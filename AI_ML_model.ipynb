{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_logs = pd.read_csv(\"synthetic_query_logs.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>bytes_scanned</th>\n",
       "      <th>rows_produced</th>\n",
       "      <th>query_text</th>\n",
       "      <th>efficiency_score</th>\n",
       "      <th>complexity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>user_c</td>\n",
       "      <td>4.775598</td>\n",
       "      <td>264.219506</td>\n",
       "      <td>812.012261</td>\n",
       "      <td>JOIN</td>\n",
       "      <td>3.073249</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>user_a</td>\n",
       "      <td>1.104920</td>\n",
       "      <td>5169.721277</td>\n",
       "      <td>868.401595</td>\n",
       "      <td>SELECT</td>\n",
       "      <td>0.167978</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>user_c</td>\n",
       "      <td>16.493767</td>\n",
       "      <td>2342.308174</td>\n",
       "      <td>914.108147</td>\n",
       "      <td>AGGREGATE</td>\n",
       "      <td>0.390260</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>user_c</td>\n",
       "      <td>14.430290</td>\n",
       "      <td>6487.210625</td>\n",
       "      <td>516.228975</td>\n",
       "      <td>JOIN</td>\n",
       "      <td>0.079576</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>user_a</td>\n",
       "      <td>14.851136</td>\n",
       "      <td>1826.227647</td>\n",
       "      <td>506.501132</td>\n",
       "      <td>AGGREGATE</td>\n",
       "      <td>0.277348</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id user_name  execution_time  bytes_scanned  rows_produced  \\\n",
       "0         1    user_c        4.775598     264.219506     812.012261   \n",
       "1         2    user_a        1.104920    5169.721277     868.401595   \n",
       "2         3    user_c       16.493767    2342.308174     914.108147   \n",
       "3         4    user_c       14.430290    6487.210625     516.228975   \n",
       "4         5    user_a       14.851136    1826.227647     506.501132   \n",
       "\n",
       "  query_text  efficiency_score  complexity_score  \n",
       "0       JOIN          3.073249                 2  \n",
       "1     SELECT          0.167978                 1  \n",
       "2  AGGREGATE          0.390260                 1  \n",
       "3       JOIN          0.079576                 2  \n",
       "4  AGGREGATE          0.277348                 1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_logs.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   query_id user_name  execution_time  bytes_scanned  rows_produced  \\\n",
      "0         1    user_c       -0.971688      -1.739819     812.012261   \n",
      "1         2    user_a       -1.644859      -0.059589     868.401595   \n",
      "2         3    user_c        1.177324      -1.028033     914.108147   \n",
      "3         4    user_c        0.798900       0.391677     516.228975   \n",
      "4         5    user_a        0.876079      -1.204801     506.501132   \n",
      "\n",
      "  query_text  efficiency_score  complexity_score  \n",
      "0       JOIN          3.073249          1.644294  \n",
      "1     SELECT          0.167978         -0.608164  \n",
      "2  AGGREGATE          0.390260         -0.608164  \n",
      "3       JOIN          0.079576          1.644294  \n",
      "4  AGGREGATE          0.277348         -0.608164  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "query_logs = pd.read_csv(\"synthetic_query_logs.csv\")\n",
    "\n",
    "# Select the columns to normalize\n",
    "features_to_normalize = ['execution_time', 'bytes_scanned', 'complexity_score']\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Normalize the features\n",
    "query_logs[features_to_normalize] = scaler.fit_transform(query_logs[features_to_normalize])\n",
    "\n",
    "# View the normalized data\n",
    "print(query_logs.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the AI Model\n",
    "* Label Data for Suboptimal Queries\n",
    "\n",
    "* Defining of suboptimal queries as those with:\n",
    "execution_time > 10 seconds\n",
    "efficiency_score < 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_logs['label'] = ((query_logs['execution_time'] > 10) | (query_logs['efficiency_score'] < 0.1)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83        12\n",
      "           1       0.75      0.75      0.75         8\n",
      "\n",
      "    accuracy                           0.80        20\n",
      "   macro avg       0.79      0.79      0.79        20\n",
      "weighted avg       0.80      0.80      0.80        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define features and target\n",
    "X = query_logs[['execution_time', 'bytes_scanned', 'complexity_score']]\n",
    "y = query_logs['label']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it mean?\n",
    "\n",
    "Precision tells you how many of the predicted positives were actually positive.\n",
    "Recall tells you how many of the actual positives were correctly identified.\n",
    "F1-Score gives a balanced measure considering both precision and recall.\n",
    "Accuracy measures overall correct predictions.\n",
    "Macro avg and Weighted avg provide averages for all classes, with the weighted average considering class imbalances.\n",
    "\n",
    "\n",
    "In this case, the model has slightly better performance for class 0 (precision and recall of 0.83) compared to class 1 (precision and recall of 0.75), which is reflected in the overall accuracy and averages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
